E:\ANACONDA\envs\YOLO_GPU\python.exe C:\turn_E\prj\VLSI\lesson\DETECT\seclect\EYES\project\resnet_data_m_train.py 
E:\ANACONDA\envs\YOLO_GPU\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
E:\ANACONDA\envs\YOLO_GPU\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/50] Batch [1/32] Loss: 1.4075 Acc: 0.1562
Epoch [1/50] Batch [2/32] Loss: 1.3657 Acc: 0.2188
Epoch [1/50] Batch [3/32] Loss: 1.3029 Acc: 0.3750
Epoch [1/50] Batch [4/32] Loss: 1.4559 Acc: 0.3750
Epoch [1/50] Batch [5/32] Loss: 1.2964 Acc: 0.4375
Epoch [1/50] Batch [6/32] Loss: 1.2753 Acc: 0.4375
Epoch [1/50] Batch [7/32] Loss: 1.4675 Acc: 0.4375
Epoch [1/50] Batch [8/32] Loss: 1.0912 Acc: 0.5625
Epoch [1/50] Batch [9/32] Loss: 1.0708 Acc: 0.5625
Epoch [1/50] Batch [10/32] Loss: 1.1735 Acc: 0.5000
Epoch [1/50] Batch [11/32] Loss: 1.0337 Acc: 0.5938
Epoch [1/50] Batch [12/32] Loss: 1.1322 Acc: 0.4375
Epoch [1/50] Batch [13/32] Loss: 1.2343 Acc: 0.5312
Epoch [1/50] Batch [14/32] Loss: 1.2508 Acc: 0.5000
Epoch [1/50] Batch [15/32] Loss: 1.0753 Acc: 0.5000
Epoch [1/50] Batch [16/32] Loss: 0.9811 Acc: 0.5312
Epoch [1/50] Batch [17/32] Loss: 1.1999 Acc: 0.5000
Epoch [1/50] Batch [18/32] Loss: 1.1426 Acc: 0.5625
Epoch [1/50] Batch [19/32] Loss: 1.1003 Acc: 0.5938
Epoch [1/50] Batch [20/32] Loss: 0.8424 Acc: 0.6250
Epoch [1/50] Batch [21/32] Loss: 1.1882 Acc: 0.5000
Epoch [1/50] Batch [22/32] Loss: 0.8507 Acc: 0.6562
Epoch [1/50] Batch [23/32] Loss: 1.0410 Acc: 0.6250
Epoch [1/50] Batch [24/32] Loss: 1.0382 Acc: 0.5938
Epoch [1/50] Batch [25/32] Loss: 1.1875 Acc: 0.5312
Epoch [1/50] Batch [26/32] Loss: 0.8007 Acc: 0.6562
Epoch [1/50] Batch [27/32] Loss: 1.0552 Acc: 0.6250
Epoch [1/50] Batch [28/32] Loss: 1.0165 Acc: 0.5938
Epoch [1/50] Batch [29/32] Loss: 0.9580 Acc: 0.5938
Epoch [1/50] Batch [30/32] Loss: 0.9356 Acc: 0.5625
Epoch [1/50] Batch [31/32] Loss: 1.3024 Acc: 0.5000
Epoch [1/50] Batch [32/32] Loss: 0.8846 Acc: 0.6667
Epoch [1/50] Summary: Train Loss: 1.1311 Acc: 0.5162 | Val Loss: 1.0308 Acc: 0.5500
Epoch [2/50] Batch [1/32] Loss: 0.6622 Acc: 0.6875
Epoch [2/50] Batch [2/32] Loss: 0.7561 Acc: 0.7188
Epoch [2/50] Batch [3/32] Loss: 0.7168 Acc: 0.7500
Epoch [2/50] Batch [4/32] Loss: 0.8985 Acc: 0.5938
Epoch [2/50] Batch [5/32] Loss: 0.6517 Acc: 0.7812
Epoch [2/50] Batch [6/32] Loss: 0.7055 Acc: 0.8125
Epoch [2/50] Batch [7/32] Loss: 0.8489 Acc: 0.5938
Epoch [2/50] Batch [8/32] Loss: 0.7869 Acc: 0.6562
Epoch [2/50] Batch [9/32] Loss: 0.5692 Acc: 0.8125
Epoch [2/50] Batch [10/32] Loss: 0.6123 Acc: 0.7188
Epoch [2/50] Batch [11/32] Loss: 0.6604 Acc: 0.7812
Epoch [2/50] Batch [12/32] Loss: 0.6254 Acc: 0.8125
Epoch [2/50] Batch [13/32] Loss: 0.6480 Acc: 0.7812
Epoch [2/50] Batch [14/32] Loss: 0.7000 Acc: 0.6562
Epoch [2/50] Batch [15/32] Loss: 0.6588 Acc: 0.8125
Epoch [2/50] Batch [16/32] Loss: 0.4899 Acc: 0.8438
Epoch [2/50] Batch [17/32] Loss: 0.8154 Acc: 0.5625
Epoch [2/50] Batch [18/32] Loss: 0.6537 Acc: 0.8438
Epoch [2/50] Batch [19/32] Loss: 0.5384 Acc: 0.7812
Epoch [2/50] Batch [20/32] Loss: 0.6384 Acc: 0.8125
Epoch [2/50] Batch [21/32] Loss: 0.5516 Acc: 0.7812
Epoch [2/50] Batch [22/32] Loss: 0.7814 Acc: 0.5938
Epoch [2/50] Batch [23/32] Loss: 0.8255 Acc: 0.6250
Epoch [2/50] Batch [24/32] Loss: 0.6261 Acc: 0.7500
Epoch [2/50] Batch [25/32] Loss: 0.6399 Acc: 0.7500
Epoch [2/50] Batch [26/32] Loss: 0.9870 Acc: 0.6562
Epoch [2/50] Batch [27/32] Loss: 0.6986 Acc: 0.6875
Epoch [2/50] Batch [28/32] Loss: 0.4894 Acc: 0.8438
Epoch [2/50] Batch [29/32] Loss: 0.7311 Acc: 0.6562
Epoch [2/50] Batch [30/32] Loss: 0.9532 Acc: 0.5625
Epoch [2/50] Batch [31/32] Loss: 0.5969 Acc: 0.7812
Epoch [2/50] Batch [32/32] Loss: 0.6274 Acc: 0.6667
Epoch [2/50] Summary: Train Loss: 0.6923 Acc: 0.7242 | Val Loss: 1.0671 Acc: 0.5944
Epoch [3/50] Batch [1/32] Loss: 0.4038 Acc: 0.9062
Epoch [3/50] Batch [2/32] Loss: 0.4321 Acc: 0.8438
Epoch [3/50] Batch [3/32] Loss: 0.4352 Acc: 0.8750
Epoch [3/50] Batch [4/32] Loss: 0.3416 Acc: 0.9062
Epoch [3/50] Batch [5/32] Loss: 0.3839 Acc: 0.9062
Epoch [3/50] Batch [6/32] Loss: 0.4341 Acc: 0.8750
Epoch [3/50] Batch [7/32] Loss: 0.3377 Acc: 0.8750
Epoch [3/50] Batch [8/32] Loss: 0.4872 Acc: 0.8750
Epoch [3/50] Batch [9/32] Loss: 0.3420 Acc: 0.9688
Epoch [3/50] Batch [10/32] Loss: 0.3958 Acc: 0.8750
Epoch [3/50] Batch [11/32] Loss: 0.3262 Acc: 0.8750
Epoch [3/50] Batch [12/32] Loss: 0.2624 Acc: 0.9688
Epoch [3/50] Batch [13/32] Loss: 0.2884 Acc: 0.9062
Epoch [3/50] Batch [14/32] Loss: 0.2723 Acc: 0.9375
Epoch [3/50] Batch [15/32] Loss: 0.2907 Acc: 0.9688
Epoch [3/50] Batch [16/32] Loss: 0.3725 Acc: 0.9062
Epoch [3/50] Batch [17/32] Loss: 0.2674 Acc: 0.9062
Epoch [3/50] Batch [18/32] Loss: 0.2837 Acc: 0.9062
Epoch [3/50] Batch [19/32] Loss: 0.4072 Acc: 0.9062
Epoch [3/50] Batch [20/32] Loss: 0.4301 Acc: 0.8438
Epoch [3/50] Batch [21/32] Loss: 0.2765 Acc: 0.9375
Epoch [3/50] Batch [22/32] Loss: 0.3340 Acc: 0.8750
Epoch [3/50] Batch [23/32] Loss: 0.3208 Acc: 0.8750
Epoch [3/50] Batch [24/32] Loss: 0.3951 Acc: 0.9062
Epoch [3/50] Batch [25/32] Loss: 0.4712 Acc: 0.7500
Epoch [3/50] Batch [26/32] Loss: 0.5961 Acc: 0.7812
Epoch [3/50] Batch [27/32] Loss: 0.3470 Acc: 0.9062
Epoch [3/50] Batch [28/32] Loss: 0.3638 Acc: 0.8750
Epoch [3/50] Batch [29/32] Loss: 0.5150 Acc: 0.8438
Epoch [3/50] Batch [30/32] Loss: 0.3400 Acc: 0.9062
Epoch [3/50] Batch [31/32] Loss: 0.2858 Acc: 0.9062
Epoch [3/50] Batch [32/32] Loss: 0.3560 Acc: 0.8148
Epoch [3/50] Summary: Train Loss: 0.3687 Acc: 0.8881 | Val Loss: 0.9645 Acc: 0.6444
Epoch [4/50] Batch [1/32] Loss: 0.1306 Acc: 1.0000
Epoch [4/50] Batch [2/32] Loss: 0.1661 Acc: 1.0000
Epoch [4/50] Batch [3/32] Loss: 0.1504 Acc: 0.9688
Epoch [4/50] Batch [4/32] Loss: 0.2365 Acc: 0.9688
Epoch [4/50] Batch [5/32] Loss: 0.2259 Acc: 0.9375
Epoch [4/50] Batch [6/32] Loss: 0.1480 Acc: 0.9688
Epoch [4/50] Batch [7/32] Loss: 0.1811 Acc: 0.9688
Epoch [4/50] Batch [8/32] Loss: 0.1845 Acc: 0.9062
Epoch [4/50] Batch [9/32] Loss: 0.1318 Acc: 1.0000
Epoch [4/50] Batch [10/32] Loss: 0.1846 Acc: 0.9688
Epoch [4/50] Batch [11/32] Loss: 0.1169 Acc: 1.0000
Epoch [4/50] Batch [12/32] Loss: 0.1204 Acc: 1.0000
Epoch [4/50] Batch [13/32] Loss: 0.1428 Acc: 0.9688
Epoch [4/50] Batch [14/32] Loss: 0.3317 Acc: 0.9062
Epoch [4/50] Batch [15/32] Loss: 0.1786 Acc: 0.9688
Epoch [4/50] Batch [16/32] Loss: 0.1561 Acc: 0.9688
Epoch [4/50] Batch [17/32] Loss: 0.0960 Acc: 1.0000
Epoch [4/50] Batch [18/32] Loss: 0.2172 Acc: 0.9375
Epoch [4/50] Batch [19/32] Loss: 0.1405 Acc: 1.0000
Epoch [4/50] Batch [20/32] Loss: 0.0670 Acc: 1.0000
Epoch [4/50] Batch [21/32] Loss: 0.1840 Acc: 0.9375
Epoch [4/50] Batch [22/32] Loss: 0.1422 Acc: 0.9688
Epoch [4/50] Batch [23/32] Loss: 0.1039 Acc: 1.0000
Epoch [4/50] Batch [24/32] Loss: 0.1371 Acc: 1.0000
Epoch [4/50] Batch [25/32] Loss: 0.0759 Acc: 1.0000
Epoch [4/50] Batch [26/32] Loss: 0.1014 Acc: 1.0000
Epoch [4/50] Batch [27/32] Loss: 0.1043 Acc: 0.9688
Epoch [4/50] Batch [28/32] Loss: 0.1282 Acc: 0.9688
Epoch [4/50] Batch [29/32] Loss: 0.1301 Acc: 0.9688
Epoch [4/50] Batch [30/32] Loss: 0.1084 Acc: 1.0000
Epoch [4/50] Batch [31/32] Loss: 0.1631 Acc: 0.9375
Epoch [4/50] Batch [32/32] Loss: 0.3306 Acc: 0.9259
Epoch [4/50] Summary: Train Loss: 0.1559 Acc: 0.9725 | Val Loss: 1.0706 Acc: 0.6167
Epoch [5/50] Batch [1/32] Loss: 0.0610 Acc: 1.0000
Epoch [5/50] Batch [2/32] Loss: 0.0572 Acc: 1.0000
Epoch [5/50] Batch [3/32] Loss: 0.0467 Acc: 1.0000
Epoch [5/50] Batch [4/32] Loss: 0.0422 Acc: 1.0000
Epoch [5/50] Batch [5/32] Loss: 0.0492 Acc: 1.0000
Epoch [5/50] Batch [6/32] Loss: 0.1110 Acc: 0.9688
Epoch [5/50] Batch [7/32] Loss: 0.1189 Acc: 0.9688
Epoch [5/50] Batch [8/32] Loss: 0.0745 Acc: 1.0000
Epoch [5/50] Batch [9/32] Loss: 0.1414 Acc: 0.9375
Epoch [5/50] Batch [10/32] Loss: 0.2566 Acc: 0.9375
Epoch [5/50] Batch [11/32] Loss: 0.0642 Acc: 1.0000
Epoch [5/50] Batch [12/32] Loss: 0.0325 Acc: 1.0000
Epoch [5/50] Batch [13/32] Loss: 0.0687 Acc: 1.0000
Epoch [5/50] Batch [14/32] Loss: 0.0546 Acc: 1.0000
Epoch [5/50] Batch [15/32] Loss: 0.0325 Acc: 1.0000
Epoch [5/50] Batch [16/32] Loss: 0.0529 Acc: 1.0000
Epoch [5/50] Batch [17/32] Loss: 0.1249 Acc: 1.0000
Epoch [5/50] Batch [18/32] Loss: 0.0484 Acc: 1.0000
Epoch [5/50] Batch [19/32] Loss: 0.0459 Acc: 1.0000
Epoch [5/50] Batch [20/32] Loss: 0.0487 Acc: 1.0000
Epoch [5/50] Batch [21/32] Loss: 0.0429 Acc: 1.0000
Epoch [5/50] Batch [22/32] Loss: 0.0830 Acc: 0.9688
Epoch [5/50] Batch [23/32] Loss: 0.0337 Acc: 1.0000
Epoch [5/50] Batch [24/32] Loss: 0.1012 Acc: 0.9688
Epoch [5/50] Batch [25/32] Loss: 0.0602 Acc: 1.0000
Epoch [5/50] Batch [26/32] Loss: 0.0455 Acc: 1.0000
Epoch [5/50] Batch [27/32] Loss: 0.0376 Acc: 1.0000
Epoch [5/50] Batch [28/32] Loss: 0.0742 Acc: 0.9688
Epoch [5/50] Batch [29/32] Loss: 0.0482 Acc: 1.0000
Epoch [5/50] Batch [30/32] Loss: 0.0378 Acc: 1.0000
Epoch [5/50] Batch [31/32] Loss: 0.0319 Acc: 1.0000
Epoch [5/50] Batch [32/32] Loss: 0.0666 Acc: 0.9630
Epoch [5/50] Summary: Train Loss: 0.0686 Acc: 0.9902 | Val Loss: 1.1754 Acc: 0.6500
Epoch [6/50] Batch [1/32] Loss: 0.0187 Acc: 1.0000
Epoch [6/50] Batch [2/32] Loss: 0.0228 Acc: 1.0000
Epoch [6/50] Batch [3/32] Loss: 0.0274 Acc: 1.0000
Epoch [6/50] Batch [4/32] Loss: 0.0313 Acc: 1.0000
Epoch [6/50] Batch [5/32] Loss: 0.0194 Acc: 1.0000
Epoch [6/50] Batch [6/32] Loss: 0.0256 Acc: 1.0000
Epoch [6/50] Batch [7/32] Loss: 0.0573 Acc: 1.0000
Epoch [6/50] Batch [8/32] Loss: 0.0350 Acc: 1.0000
Epoch [6/50] Batch [9/32] Loss: 0.0204 Acc: 1.0000
Epoch [6/50] Batch [10/32] Loss: 0.0226 Acc: 1.0000
Epoch [6/50] Batch [11/32] Loss: 0.0309 Acc: 1.0000
Epoch [6/50] Batch [12/32] Loss: 0.0228 Acc: 1.0000
Epoch [6/50] Batch [13/32] Loss: 0.0399 Acc: 1.0000
Epoch [6/50] Batch [14/32] Loss: 0.0135 Acc: 1.0000
Epoch [6/50] Batch [15/32] Loss: 0.0138 Acc: 1.0000
Epoch [6/50] Batch [16/32] Loss: 0.0154 Acc: 1.0000
Epoch [6/50] Batch [17/32] Loss: 0.0210 Acc: 1.0000
Epoch [6/50] Batch [18/32] Loss: 0.0151 Acc: 1.0000
Epoch [6/50] Batch [19/32] Loss: 0.0360 Acc: 1.0000
Epoch [6/50] Batch [20/32] Loss: 0.0170 Acc: 1.0000
Epoch [6/50] Batch [21/32] Loss: 0.0147 Acc: 1.0000
Epoch [6/50] Batch [22/32] Loss: 0.0196 Acc: 1.0000
Epoch [6/50] Batch [23/32] Loss: 0.0290 Acc: 1.0000
Epoch [6/50] Batch [24/32] Loss: 0.0292 Acc: 1.0000
Epoch [6/50] Batch [25/32] Loss: 0.1019 Acc: 1.0000
Epoch [6/50] Batch [26/32] Loss: 0.0501 Acc: 1.0000
Epoch [6/50] Batch [27/32] Loss: 0.1024 Acc: 0.9688
Epoch [6/50] Batch [28/32] Loss: 0.0143 Acc: 1.0000
Epoch [6/50] Batch [29/32] Loss: 0.0352 Acc: 1.0000
Epoch [6/50] Batch [30/32] Loss: 0.0203 Acc: 1.0000
Epoch [6/50] Batch [31/32] Loss: 0.0093 Acc: 1.0000
Epoch [6/50] Batch [32/32] Loss: 0.0176 Acc: 1.0000
Epoch [6/50] Summary: Train Loss: 0.0297 Acc: 0.9990 | Val Loss: 1.4960 Acc: 0.6333
Epoch [7/50] Batch [1/32] Loss: 0.0123 Acc: 1.0000
Epoch [7/50] Batch [2/32] Loss: 0.0301 Acc: 1.0000
Epoch [7/50] Batch [3/32] Loss: 0.0283 Acc: 1.0000
Epoch [7/50] Batch [4/32] Loss: 0.0562 Acc: 1.0000
Epoch [7/50] Batch [5/32] Loss: 0.0332 Acc: 1.0000
Epoch [7/50] Batch [6/32] Loss: 0.0043 Acc: 1.0000
Epoch [7/50] Batch [7/32] Loss: 0.0298 Acc: 1.0000
Epoch [7/50] Batch [8/32] Loss: 0.0065 Acc: 1.0000
Epoch [7/50] Batch [9/32] Loss: 0.0161 Acc: 1.0000
Epoch [7/50] Batch [10/32] Loss: 0.0206 Acc: 1.0000
Epoch [7/50] Batch [11/32] Loss: 0.0196 Acc: 1.0000
Epoch [7/50] Batch [12/32] Loss: 0.0267 Acc: 1.0000
Epoch [7/50] Batch [13/32] Loss: 0.0197 Acc: 1.0000
Epoch [7/50] Batch [14/32] Loss: 0.0176 Acc: 1.0000
Epoch [7/50] Batch [15/32] Loss: 0.0318 Acc: 1.0000
Epoch [7/50] Batch [16/32] Loss: 0.0269 Acc: 1.0000
Epoch [7/50] Batch [17/32] Loss: 0.0140 Acc: 1.0000
Epoch [7/50] Batch [18/32] Loss: 0.0161 Acc: 1.0000
Epoch [7/50] Batch [19/32] Loss: 0.0952 Acc: 0.9688
Epoch [7/50] Batch [20/32] Loss: 0.0098 Acc: 1.0000
Epoch [7/50] Batch [21/32] Loss: 0.0096 Acc: 1.0000
Epoch [7/50] Batch [22/32] Loss: 0.0437 Acc: 0.9688
Epoch [7/50] Batch [23/32] Loss: 0.0089 Acc: 1.0000
Epoch [7/50] Batch [24/32] Loss: 0.0221 Acc: 1.0000
Epoch [7/50] Batch [25/32] Loss: 0.0245 Acc: 1.0000
Epoch [7/50] Batch [26/32] Loss: 0.0076 Acc: 1.0000
Epoch [7/50] Batch [27/32] Loss: 0.0103 Acc: 1.0000
Epoch [7/50] Batch [28/32] Loss: 0.0376 Acc: 1.0000
Epoch [7/50] Batch [29/32] Loss: 0.1327 Acc: 0.9375
Epoch [7/50] Batch [30/32] Loss: 0.0088 Acc: 1.0000
Epoch [7/50] Batch [31/32] Loss: 0.0190 Acc: 1.0000
Epoch [7/50] Batch [32/32] Loss: 0.0072 Acc: 1.0000
Epoch [7/50] Summary: Train Loss: 0.0266 Acc: 0.9961 | Val Loss: 1.4019 Acc: 0.6111
Epoch [8/50] Batch [1/32] Loss: 0.0134 Acc: 1.0000
Epoch [8/50] Batch [2/32] Loss: 0.0139 Acc: 1.0000
Epoch [8/50] Batch [3/32] Loss: 0.0085 Acc: 1.0000
Epoch [8/50] Batch [4/32] Loss: 0.0154 Acc: 1.0000
Epoch [8/50] Batch [5/32] Loss: 0.0119 Acc: 1.0000
Epoch [8/50] Batch [6/32] Loss: 0.0376 Acc: 1.0000
Epoch [8/50] Batch [7/32] Loss: 0.0189 Acc: 1.0000
Epoch [8/50] Batch [8/32] Loss: 0.0886 Acc: 0.9688
Epoch [8/50] Batch [9/32] Loss: 0.0440 Acc: 1.0000
Epoch [8/50] Batch [10/32] Loss: 0.0175 Acc: 1.0000
Epoch [8/50] Batch [11/32] Loss: 0.0120 Acc: 1.0000
Epoch [8/50] Batch [12/32] Loss: 0.0437 Acc: 1.0000
Epoch [8/50] Batch [13/32] Loss: 0.0323 Acc: 1.0000
Epoch [8/50] Batch [14/32] Loss: 0.0149 Acc: 1.0000
Epoch [8/50] Batch [15/32] Loss: 0.0277 Acc: 1.0000
Epoch [8/50] Batch [16/32] Loss: 0.0108 Acc: 1.0000
Epoch [8/50] Batch [17/32] Loss: 0.0093 Acc: 1.0000
Epoch [8/50] Batch [18/32] Loss: 0.0279 Acc: 1.0000
Epoch [8/50] Batch [19/32] Loss: 0.0259 Acc: 1.0000
Epoch [8/50] Batch [20/32] Loss: 0.0964 Acc: 0.9688
Epoch [8/50] Batch [21/32] Loss: 0.0131 Acc: 1.0000
Epoch [8/50] Batch [22/32] Loss: 0.0069 Acc: 1.0000
Epoch [8/50] Batch [23/32] Loss: 0.0468 Acc: 1.0000
Epoch [8/50] Batch [24/32] Loss: 0.0182 Acc: 1.0000
Epoch [8/50] Batch [25/32] Loss: 0.0687 Acc: 0.9688
Epoch [8/50] Batch [26/32] Loss: 0.0131 Acc: 1.0000
Epoch [8/50] Batch [27/32] Loss: 0.0247 Acc: 1.0000
Epoch [8/50] Batch [28/32] Loss: 0.0079 Acc: 1.0000
Epoch [8/50] Batch [29/32] Loss: 0.0122 Acc: 1.0000
Epoch [8/50] Batch [30/32] Loss: 0.0078 Acc: 1.0000
Epoch [8/50] Batch [31/32] Loss: 0.0084 Acc: 1.0000
Epoch [8/50] Batch [32/32] Loss: 0.0364 Acc: 1.0000
Epoch [8/50] Summary: Train Loss: 0.0260 Acc: 0.9971 | Val Loss: 1.4359 Acc: 0.6222
Early stopping at epoch 8
训练完成！
模型已保存为 resnet_eye_model.pth

Process finished with exit code 0
